<!DOCTYPE html>

<head>
	<title>Drake Thomas</title>
	<link rel="stylesheet" type="text/css" href="css/main.css">
	<script src="https://code.jquery.com/jquery-1.10.2.js"></script>


	<!-- Global site tag (gtag.js) - Google Analytics -->
	<!--
	Omitting this because it's sending analytics to Eli's page
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-205493387-1"></script>
	<script>
  	 window.dataLayer = window.dataLayer || [];
  	 function gtag(){dataLayer.push(arguments);}
  	 gtag('js', new Date());

  	 gtag('config', 'UA-205493387-1');
	</script>
	-->


</head>


<body>
	<!--Navigation bar-->
	<div id="nav-placeholder">
	</div>
	<!--This /\ is where the Nav bar goes on the page.
	this \/ is the code that goes and gets the nav bar content from nav.html-->
	<script>
	$(function(){
	  $("#nav-placeholder").load("nav.html");
	});
	</script>
	<!--end of Navigation bar-->

	<div class="page title">
		<h1>Mistakes</h1>
	</div>


<div class="main text">

	<p>
		I am often wrong about things! I'd like to notice this happening more, so I'm tracking the more important instances here in the interests of having a better record and transmitting things I've learned to readers of this page.
	</p>
	<p>
	Where I can recall the source that led me to correct my opinions, I've tried to cite them, though I make no guarantees that my second-hand transmission of the things they pointed out will be fully accurate or endorsed by the original speakers.
	</p>
	<p>
	I started this page in mid-December 2022; entries from before then are sparser and fuzzily recalled.
	</p>


	<h2>Nontrivial mistakes and confusions</h2>
	<ul>
		<li>
			[2023-08] A friend argued to me about the parallels between abstaining from factory-farmed meat and offsetting carbon emissions (in that both are a case of personal sacrifice to avoid directly contributing to a large-scale problem which is probably more effectively solved by donating money), making the case that it wasn't particularly consistent for me to care a lot about the first and not about the second. I haven't changed my actions on either front much, but I think it's probably right to say that my decision-making with respect to vegetarianism is substantially influenced by a social environment that rewards it, and that I should have at least tried to run the numbers on both cases to figure out the magnitude of the resulting harms. 
		</li>
		<li>
			[2023-04] I was still confused about the tradeoffs of optimization! Unlike the situation in the 2022-12-16 bullet point, there are joint X/Y distributions for which more extreme X-optimization produces less Y, if for instance X is modeled as Y plus an independent error term that is heavy-tailed and with heavier tails than Y. Intuition and exposition of this phenomenon is at <a href="https://www.lesswrong.com/posts/nnDTgmzRrzDMiPF9B/how-much-do-you-believe-your-results">this post</a> of Eric Neyman's, and Thomas Kwa and I worked out some math on when this behavior manifests <a href="https://www.lesswrong.com/posts/fuSaKr6t6Zuh6GKaQ/when-is-goodhart-catastrophic">here</a>. Thanks to Thomas Kwa for bringing my attention to this mathematical model and working with me on piecing together its subtleties. My take on this situation is now "things are kind of subtle and complicated and you have to just actually think about the precise situation at hand; no pithy slogans apply."
		</li>
		<li>
			[2023-03-01] Joe Carlsmith's essay <a href="https://joecarlsmith.com/2022/01/30/on-infinite-ethics">on infinite ethics</a> clarified an issue about choice of moral "location" that I hadn't clarified for myself before.
		</li>
		<li>
			[2023-02] I now think that the Center for Effective Altruism is taking substantially more reasonable actions given the constraints they are operating under than e.g. many critics on the EA forum do. (This was a result of some reflection and talking with friends who had more context on )
		</li>
		<li>
			[2023-01] Updated in favor of "doom circles" being a social technology to be pretty cautious of, and one that has risky failure modes if not run by someone well aware of them and planning for how to avoid them. I am now inclined not to join cavalier doom circles in a way I wasn't before. (This was from a combination of direct experience and conversations with friends.)
		</li>
		<li>
			[2023-01-17] I'd often seen arguments about superexponential GDP progress and considered them (moderately weak) evidence for the future being pretty crazy. I now think these arguments aren't as strong, because while some of the GDP growth is due to Mysterious Forces of Progress we don't understand, a good piece of it is straightforwardly from population growth, and that's started to level off in predictable ways, so we can do better forecasting than blindly extrapolating the line on the graph by accounting for a stabilizing world population over the coming decades. I now think GDP per capita is likely a better thing to plot for this sort of analysis. h/t Charlotte Siegmann for first making this point to me.
		</li>
		<li>
			[2022-12-16] I had often made analogies between interpretability work in AI and fields of science like biology; I now think this isn't a great analogy, because of the extent to which researching neural networks involves complete access to the internal dynamics of the system, infallible measurement, replicable causal interventions, etc. Thanks to Buck Schlegeris for pointing this out to me.
		</li>
		<li>
			[2022-12-16] I was confused about the tradeoffs of optimization.
			Suppose that X and Y are normally-distributed variables that are correlated, but not perfectly so.
			I thought that if I optimized hard enough for X (say, by sampling from this distribution until I see a datapoint with an X-value over some threshold), this would eventually start trading off against the value of Y. But actually, the expected Y-value just scales linearly with the expected X-value; I drift off of the major axis of the ellipse in a scatterplot, but the slope isn't negative. Thanks to Eric Neyman for this observation.
		</li>
		<li>
			[December 2022] Updated towards worrying more about how path-dependent human value formation is, how much something like CEV is well-defined, the monotonicity of human morality improvement over time, and whether the future will mostly contain agents working towards goals that I endorse on net. (This was a process of some conversations with assorted friends.)
		</li>
		<li>
			[November 2022] I had heard some third-hand tales of poor behavior on the part of people involved in FTX back in 2021, thought that I should have more skepticism and less trust of them as a result, and then observed this skepticism gradually recede over time for no good reason. In retrospect I should have maintained it.
		</li>
	 	<li>
			[Late 2021 / Early 2022] I had not really considered that perhaps I should <i>check</i> whether the AI alignment research
		  	I intellectually endorsed people working on was something I might personally want to do or be any good at.
	  </li>
	</ul>

	<h2>Minor errors and misconceptions</h2>
	<ul>
		<li>
			[2023-08] I hadn't realized that urine is just filtered blood? In retrospect, I knew the kidneys were involved in urine production and in blood filtration, and didn't have any beliefs about a pipeline out of the human stomach to siphon away liquid, so it seems sort of obvious in retrospect, but I'd never thought about it before.
		</li>
		<li>
			[2022-12-25] I thought that transformer language models were bottlenecked on having larger context windows because this required them to learn many more parameters, but in fact the architecture has very few parameters that scale with the context window (just the position encoding, which is a small fraction of the parameters in existing models and for some architectures is hard-coded). The bottleneck as I now understand it is that running a forward pass scales quadratically with context, so for both training and inference you have to run it for longer.
		</li>
		<li>
			[2022] I thought that the "stochastic" in SGD (stochastic gradient descent) referred to taking steps that randomly varied from the direction of steepest gradient as measured by the loss on the most recent batch (so as to escape local minima or something). Actually this is not how SGD works, and the "stochastic" part is just in the sense that the gradient of loss on a single batch is a noisy measurement of the true gradient you would get if you computed the loss over your entire training dataset. The steps in SGD don't have random noise added on top.
		</li>
	</ul>


	<div id="last-updated"></div>



	<script>

		// this part gets the file name from the URL.
		var urlArray = window.location.href.split("/");

		var nameOfThisPage = urlArray[urlArray.length-1];

		// this part uses the github API to get the last commit data for that file, and then writes a line the HTML body.

		// future Eli, this is a function. It is in Arrow syntax. You should learn this.
		const getCommits = async pageName => {

			var result = await fetch('https://api.github.com/repos/drake-thomas/drake-thomas.github.io/commits?path='+ encodeURIComponent(pageName));

			var data = await result.json();

			var timestamp = new Date(data[0].commit.author.date)
			// the Date function takes a date string, and returns a date object.

			//var month = timestamp.getMonth()


			var formattedDate = timestamp.toDateString()

			console.log(formattedDate)




			document.getElementById('last-updated').innerHTML = "<p><br><i>This page was last updated on "+ formattedDate + ".</i></p>";


			}

			// const data = await getCommits('test');
			// Make some notes about Await and what it does.
			// Note that you can only use await inside of an async function.

			getCommits(nameOfThisPage);

		</script>
	</div>
</body>
